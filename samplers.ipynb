{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "phantom-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "import abc\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import functools\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-approach",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intelligent-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grayscale_image_as_numpy_array(path):\n",
    "    return np.asarray(Image.open(path).convert(mode='L')).astype(np.float32)\n",
    "\n",
    "\n",
    "def load_binary_image_as_numpy_array(path):\n",
    "    image = load_grayscale_image_as_numpy_array(path)\n",
    "    return 255.0 * (image > 127.5)\n",
    "\n",
    "\n",
    "def show_image(image, figsize=(12, 8), cmap='gray'):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.tick_params(labelbottom=False, labelleft=False)\n",
    "    plt.imshow(image, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spare-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_ising_model(input_image):\n",
    "    return np.pad(input_image, pad_width=1, mode='symmetric')\n",
    "\n",
    "\n",
    "def unpad_ising_model(input_image):\n",
    "    return input_image[1:-1, 1:-1]\n",
    "\n",
    "\n",
    "def convert_image_to_ising_model(input_image):\n",
    "    return pad_ising_model(input_image / 127.5 - 1.0)\n",
    "\n",
    "\n",
    "def convert_ising_model_to_image(input_image):\n",
    "    return unpad_ising_model((input_image + 1.0) * 127.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-complaint",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "incomplete-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY_IMAGES_PATH = \"binary_images/300x300_easy_noise10\"\n",
    "GRAYSCALE_IMAGES_PATH = \"grayscale_images/size800_noise10_independent_distance100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "incoming-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY_IMAGE_OBSEVATION = load_binary_image_as_numpy_array(\n",
    "    path=os.path.join(BINARY_IMAGES_PATH, \"image_0_observation.png\")\n",
    ")\n",
    "BINARY_IMAGE_GROUND_TRUTH = load_binary_image_as_numpy_array(\n",
    "    path=os.path.join(BINARY_IMAGES_PATH, \"image_0_ground_truth.png\")\n",
    ")\n",
    "GRAYSCALE_IMAGE_OBSERVATION = load_grayscale_image_as_numpy_array(\n",
    "    path=os.path.join(GRAYSCALE_IMAGES_PATH, \"image_1_observation.png\")\n",
    ")\n",
    "GRAYSCALE_IMAGE_GROUND_TRUTH = load_grayscale_image_as_numpy_array(\n",
    "    path=os.path.join(GRAYSCALE_IMAGES_PATH, \"image_1_ground_truth.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-clear",
   "metadata": {},
   "source": [
    "## Interface for noise reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "respected-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NoiseReducerStatistics(object):\n",
    "    evaluation_losses: List[float] = dataclasses.field(default_factory=list)\n",
    "    computation_times: List[float] = dataclasses.field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NoiseReducerResult(object):\n",
    "    original_image: np.ndarray\n",
    "    obsevation: np.ndarray\n",
    "    reduced_image: np.ndarray\n",
    "    statistics: NoiseReducerStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "occasional-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedNoiseReducer(object):\n",
    "    def __init__(self, iterations_count, iterations_per_evaluation):\n",
    "        self._iterations_count = iterations_count\n",
    "        self._iterations_per_evaluation = iterations_per_evaluation\n",
    "        self._average_statistics = NoiseReducerStatistics()\n",
    "\n",
    "    @property\n",
    "    def average_statistics(self):\n",
    "        return self._average_statistics\n",
    "\n",
    "    def _preprocess(self, observation):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _sampler_step(self, observation, current_state):\n",
    "        pass\n",
    "    \n",
    "    def _evaluate_noise_reduction(self, original_image, reduced_image):\n",
    "        return np.mean(np.abs(original_image - reduced_image))\n",
    "    \n",
    "    def _update_average_statistics(self, image_statistics):\n",
    "        evaluation_loss = image_statistics.evaluation_losses[-1]\n",
    "        self.average_statistics.evaluation_losses.append(evaluation_loss)\n",
    "        average_computation_time = np.mean(image_statistics.computation_times)\n",
    "        self.average_statistics.computation_times.append(average_computation_time)        \n",
    "\n",
    "    def reduce_noise(self, original_image, observation):\n",
    "        initial_state = convert_image_to_ising_model(observation)\n",
    "        current_state = initial_state.copy()\n",
    "        statistics = NoiseReducerStatistics()\n",
    "        self._preprocess(initial_state)\n",
    "        for iteration in range(1, self._iterations_count + 1):\n",
    "            start_time = time.time()          \n",
    "            self._sampler_step(initial_state, current_state)\n",
    "            statistics.computation_times.append(1000 * (time.time() - start_time))\n",
    "            if iteration % self._iterations_per_evaluation == 0:\n",
    "                statistics.evaluation_losses.append(self._evaluate_noise_reduction(\n",
    "                    original_image=original_image,                    \n",
    "                    reduced_image=convert_ising_model_to_image(current_state),\n",
    "                ))\n",
    "        self._update_average_statistics(statistics)\n",
    "        return NoiseReducerResult(\n",
    "            original_image=original_image,\n",
    "            obsevation=observation,\n",
    "            reduced_image=convert_ising_model_to_image(current_state),\n",
    "            statistics=statistics,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-decimal",
   "metadata": {},
   "source": [
    "## BinaryGibbsNoiseReducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "individual-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryGibbsNoiseReducer(SupervisedNoiseReducer):\n",
    "    def __init__(self, noise_level_prior, observation_strength, coupling_strength, **kwargs):\n",
    "        self._noise_level_prior = noise_level_prior\n",
    "        self._observation_strength = observation_strength\n",
    "        self._coupling_strength = coupling_strength\n",
    "        super(BinaryGibbsNoiseReducer, self).__init__(**kwargs)\n",
    "\n",
    "    def _observation_potential(self, observation_pbty):\n",
    "        return self._observation_strength * np.log(observation_pbty)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def _positive_observation_potential(self, observation_value):\n",
    "        if observation_value >= 0.0:\n",
    "            return self._observation_potential(1.0 - self._noise_level_prior)\n",
    "        return self._observation_potential(self._noise_level_prior)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)    \n",
    "    def _negative_observation_potential(self, observation_value):\n",
    "        if observation_value < 0.0:\n",
    "            return self._observation_potential(1.0 - self._noise_level_prior)\n",
    "        return self._observation_potential(self._noise_level_prior)\n",
    "\n",
    "    def _sampler_step(self, observation, current_state):\n",
    "        row = np.random.randint(low=1, high=(current_state.shape[0] - 1))\n",
    "        column = np.random.randint(low=1, high=(current_state.shape[1] - 1))\n",
    "        neighbours_sum = np.sum([\n",
    "            current_state[row - 1, column], current_state[row + 1, column],\n",
    "            current_state[row, column - 1], current_state[row, column + 1],\n",
    "        ])\n",
    "        positive_observation_potential = self._positive_observation_potential(\n",
    "            observation[row, column]\n",
    "        )\n",
    "        negative_observation_potential = self._negative_observation_potential(\n",
    "            observation[row, column]\n",
    "        )\n",
    "        positive_coupling = self._coupling_strength * 2.0 * neighbours_sum\n",
    "        negative_coupling = self._coupling_strength * 2.0 * -neighbours_sum\n",
    "        positive_potential = np.exp(positive_observation_potential + positive_coupling)\n",
    "        negative_potential = np.exp(negative_observation_potential + negative_coupling)\n",
    "        positive_pbty = positive_potential / (positive_potential + negative_potential)\n",
    "        if np.random.uniform() <= positive_pbty:\n",
    "            current_state[row, column] = 1.0\n",
    "        else:\n",
    "            current_state[row, column] = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "hearing-petersburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoiseReducerStatistics(evaluation_losses=[0.374], computation_times=[0.03513889416058858])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducer = BinaryGibbsNoiseReducer(\n",
    "    noise_level_prior=0.1,\n",
    "    observation_strength=1.0,\n",
    "    coupling_strength=4.0,\n",
    "    iterations_count=3_000_000,\n",
    "    iterations_per_evaluation=100,\n",
    ")\n",
    "reducer_result = reducer.reduce_noise(BINARY_IMAGE_GROUND_TRUTH, BINARY_IMAGE_OBSEVATION)\n",
    "reducer.average_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "aggressive-roads",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(reducer_result.reduced_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-representative",
   "metadata": {},
   "source": [
    "## GradientBasedNoiseReducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "occupational-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BucketBasedSampler(object):\n",
    "    def __init__(self):\n",
    "        self._buckets = defaultdict(list)\n",
    "        self._positions = {}\n",
    "\n",
    "    def _remove_key(self, key):\n",
    "        bucket_key, index_in_bucket  = self._positions[key]\n",
    "        del self._positions[key]\n",
    "        bucket = self._buckets[bucket_key]\n",
    "        bucket[index_in_bucket], bucket[-1] = bucket[-1], bucket[index_in_bucket]\n",
    "        bucket.pop()\n",
    "        if len(bucket) > index_in_bucket:\n",
    "            self._positions[bucket[index_in_bucket]] = (bucket_key, index_in_bucket)\n",
    "\n",
    "    def set_value(self, key, value):\n",
    "        if key in self._positions:\n",
    "            self._remove_key(key)\n",
    "        self._buckets[value].append(key)\n",
    "        self._positions[key] = (value, len(self._buckets[value]) - 1)\n",
    "    \n",
    "    def sample_key(self):\n",
    "        bucket_index_to_key = dict(enumerate(self._buckets.keys()))\n",
    "        bucket_probs = np.zeros(shape=(len(self._buckets), ))\n",
    "        for bucket_index, bucket_key in bucket_index_to_key.items():\n",
    "            bucket_probs[bucket_index] = bucket_key * len(self._buckets[bucket_key])\n",
    "        bucket_probs /= np.sum(bucket_probs)\n",
    "        sampled_bucket_index = np.random.choice(len(self._buckets), p=bucket_probs)\n",
    "        sampled_bucket = self._buckets[bucket_index_to_key[sampled_bucket_index]]\n",
    "        key_index = np.random.randint(low=0, high=len(sampled_bucket))\n",
    "        return sampled_bucket[key_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "renewable-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBasedNoiseReducer(SupervisedNoiseReducer):\n",
    "    def __init__(\n",
    "        self, noise_level_prior, observation_strength,\n",
    "        coupling_strength, temperature, **kwargs\n",
    "    ):\n",
    "        super(GradientBasedNoiseReducer, self).__init__(**kwargs)        \n",
    "        self._noise_level_prior = noise_level_prior\n",
    "        self._observation_strength = observation_strength\n",
    "        self._coupling_strength = coupling_strength\n",
    "        if temperature < 2.0:\n",
    "            raise ValueError(f\"Expected temperature value >= 2.0, got {temperature}\")\n",
    "        self._temperature = temperature\n",
    "        self.neighbours_sampler = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_opposite_value(value):\n",
    "        if value == 1.0:\n",
    "            return -1.0\n",
    "        elif value == -1.0:\n",
    "            return 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid value: {value}\")\n",
    "    \n",
    "    def _get_state_potential(self, observation_value, state_value):\n",
    "        if observation_value == state_value:\n",
    "            return self._observation_strength * np.log(1.0 - self._noise_level_prior)\n",
    "        return self._observation_strength * np.log(self._noise_level_prior)\n",
    "\n",
    "    def _update_sampler_state(self, row, column, current_state, observation_value, next_value):\n",
    "        if row <= 0 or row >= current_state.shape[0] - 1:\n",
    "            return\n",
    "        if column <= 0 or column >= current_state.shape[1] - 1:\n",
    "            return\n",
    "        neighbours_sum = np.sum([\n",
    "            current_state[row - 1, column], current_state[row + 1, column],\n",
    "            current_state[row, column - 1], current_state[row, column + 1],\n",
    "        ])\n",
    "        current_value = current_state[row, column]\n",
    "        values_difference = next_value - current_value\n",
    "        coupling_potential = values_difference * self._coupling_strength * neighbours_sum\n",
    "        next_state_potential = self._get_state_potential(observation_value, next_value)\n",
    "        current_state_potential = self._get_state_potential(observation_value, current_value)\n",
    "        state_potential = next_state_potential - current_state_potential\n",
    "        total_potential = np.exp(self._temperature * (coupling_potential + state_potential))\n",
    "        self.neighbours_sampler.set_value(key=(row, column), value=total_potential)\n",
    "        \n",
    "    def _preprocess(self, observation):\n",
    "        self.neighbours_sampler = BucketBasedSampler()\n",
    "        for row in range(observation.shape[0]):\n",
    "            for column in range(observation.shape[1]):\n",
    "                observation_value = observation[row, column]\n",
    "                next_value = self._get_opposite_value(observation_value)\n",
    "                self._update_sampler_state(\n",
    "                    row, column, observation, observation_value, next_value,\n",
    "                )\n",
    "\n",
    "    def _sampler_step(self, observation, current_state):\n",
    "        sampled_row, sampled_column = self.neighbours_sampler.sample_key()\n",
    "        current_state[sampled_row, sampled_column] = self._get_opposite_value(\n",
    "            current_state[sampled_row, sampled_column]\n",
    "        )\n",
    "        positions_to_update = [\n",
    "            (sampled_row, sampled_column), (sampled_row - 1, sampled_column),\n",
    "            (sampled_row + 1, sampled_column), (sampled_row, sampled_column - 1),\n",
    "            (sampled_row, sampled_column + 1),\n",
    "        ]\n",
    "        for row, column in positions_to_update:\n",
    "            observation_value = observation[row, column]\n",
    "            next_value = self._get_opposite_value(current_state[row, column])\n",
    "            self._update_sampler_state(\n",
    "                row, column, current_state, observation_value, next_value,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "alone-temperature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoiseReducerStatistics(evaluation_losses=[0.34], computation_times=[0.21001203060150148])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducer = GradientBasedNoiseReducer(\n",
    "    noise_level_prior=0.1,\n",
    "    observation_strength=1.0,\n",
    "    coupling_strength=1.0,    \n",
    "    iterations_count=30_000,\n",
    "    iterations_per_evaluation=100,\n",
    "    temperature = 2.0\n",
    ")\n",
    "reducer_result = reducer.reduce_noise(BINARY_IMAGE_GROUND_TRUTH, BINARY_IMAGE_OBSEVATION)\n",
    "reducer.average_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "precise-athletics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(reducer_result.reduced_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-country",
   "metadata": {},
   "source": [
    "## Xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-latino",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
