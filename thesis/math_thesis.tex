\documentclass[a4paper, 11pt, onecolumn, openany, titlepage]{report}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{filecontents}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{breakcites}
\usepackage{graphicx}
\graphicspath{{qualitative-results/}{quantitative-results/}}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{titlesec}
\usepackage{tabularx}
\usepackage{afterpage}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage[rightcaption]{sidecap}
\usepackage[usenames, dvipsnames]{xcolor}
\usepackage[nottoc]{tocbibind}
\usepackage[section]{placeins}
\usepackage{float}
\usepackage[parfill]{parskip}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}

\geometry{a4paper, headsep=1.0cm, footskip=1cm, lmargin=3cm, rmargin=3cm, tmargin=3cm, bmargin=3cm}
\hypersetup{colorlinks, citecolor=NavyBlue, filecolor=NavyBlue, linkcolor=NavyBlue, urlcolor=NavyBlue}

\titleformat{\chapter}[block]{\normalfont\huge\bfseries}{\thechapter.}{5pt}{\huge}
\titlespacing*{\chapter}{0pt}{-19pt}{25pt}
\titleformat{\section}[block]{\normalfont\Large\bfseries}{\thesection.}{5pt}{\Large}

\newcommand\blankpage{\null\thispagestyle{empty}\newpage}
\newcommand\numberedchapter[1]{\setlength\topskip{3cm}\chapter{#1}\setlength\topskip{0cm}}
\newcommand\unnumberedchapter[1]{\setlength\topskip{3cm}\chapter*{#1}\setlength\topskip{0cm}}

\newtheoremstyle{default_theorem_style}
  {1em plus .2em minus .1em}%   Space above
  {1em plus .2em minus .1em}%   Space below
  {\slshape}%  Body font
  {}%          Indent amount (empty = no indent, \parindent = para indent)
  {\bfseries}% Thm head font
  {.}%         Punctuation after thm head
  {0.5em}%     Space after thm head: " " = normal interword space;
     %         \newline = linebreak
  {}%          Thm head spec (can be left empty, meaning `normal')


\theoremstyle{default_theorem_style}\newtheorem{theorem}{Theorem}
\theoremstyle{default_theorem_style}\newtheorem{definition}{Definition}

\begin{document}
\setlength\topskip{3cm}
\newpage
\thispagestyle{empty}
\begin{center}
\textbf{\large Uniwersytet Wrocławski\\
Wydział Matematyki i Informatyki\\
Instytut Matematyczny}\\
\vspace{4cm}
\textbf{\textit{\large Dawid Wegner}\\
\vspace{0.5cm}
{\Large Efektywne odszumianie obrazów przy użyciu algorytmu Metropolis-Hastings}}\\
\end{center}
\vspace{3cm}
{\large \hspace*{6.5cm}Praca licencjacka\\
\hspace*{6.5cm}napisana pod kierunkiem\\
\hspace*{6.5cm}dr hab. Pawła Lorka}\\
\vfill
\begin{center}
{\large Wrocław 2021}\\
\end{center}
\setlength\topskip{0cm}
\afterpage{\blankpage}

{\hypersetup{linkcolor=black}
\setlength\topskip{3cm}
\tableofcontents
\setlength\topskip{0cm}
}

\unnumberedchapter{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

TODO

\numberedchapter{Markov Chains}

In this chapter we will formally define a Markov Chain along with its basic properties.\ Particularly, we will define
a stationary distribution and reversible Markov Chains.\ These tools are necessary to understand the theory behind
Markov Chain Monte Carlo methods that are described in the next chapter.

\section{Basic properties}

Formally, a Markov Chain is a stochastic process in which the probability of the next event depends only on the current
state.\ Intuitively, we can think of such process as a directed graph with loops.\ An example of a Markov Chain that
describes weather conditions is shown in Figure \ref{fig:markov_chain}.\ In this case, our state space contains three
elements: $\{Sun,\ Cloudy,\ Rain\}$.\ Each directed edge represents the probability of transitioning from a state
$X_{n}$ to a state $X_{n + 1}$.\ In the example presented in Figure \ref{fig:markov_chain}, the probability that the
weather will be $rainy$ in the day $X_{n + 1}$, conditioned on the fact that it was $cloudy$ in the day $X_n$, is
equal to $0.5$.\ Note that the described process carries an underlying assumption that the weather in the next
day depends only on the weather in the previous day.

\hspace*{-1.5in}
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{markov_chain}
\caption{An example of a Markov Chain describing weather conditions.}
\label{fig:markov_chain}
\end{figure}
\hspace*{-1.5in}

While a graph representation of a Markov Chain is useful, in most cases it's convenient to think of it as a matrix.
Specifically, an element $0 \leq P_{ij} \leq 1$ of a matrix $P$ is defined as the probability of transitioning from
the state $i$ to the state $j$.\ As the $i$-th row of such matrix defines probabilities of transitioning from the
$i$-th  state to all other states, it holds $\sum_{j = 1}^{n} P_{ij} = 1$.\ A matrix with such properties is called a
$stochastic$ matrix.\ In the next sections, we will think of a Markov Chain as a $stochastic$ matrix.

\begin{definition}
A vector $\mu^{(0)} = (\mu^{(0)}_1, \mu^{(0)}_2, \dots, \mu^{(0)}_n)$ such that $\sum_{i=1}^{n} \mu^{(0)}_i = 1$
is called an $initial\ state$ of a Markov Chain.\ It defines an initial distribution across the state space.

\end{definition}

The Markov Chain process is fully characterized by a transition matrix $P$ and an initial state $\mu^{(0)}$.\ In
particular, the distribution of a Markov Chain in the $k$-th step can be uniquely determined by utilising the
formula $\mu^{(k)} = \mu^{(0)}P^k$.\ Although a Markov Chain can contain infinitely many states, we only consider
Markov Chains with a finite number of states.\ In particular, some theorems presented in the next sections
are not true without this assumption.\newline

\section{Stationary distribution}

In this section we will formalize a notion of a $stationary$ distribution and explore its properties.\ Furthermore,
we will provide sufficient conditions to guarantee that a Markov Chain has a unique $stationary$ distribution.

\begin{definition}
A vector $\pi = (\pi_1, \pi_2, \dots, \pi_n)$ is called a $stationary$ distribution iff $\pi P = \pi$, where $P$
denotes a transition matrix of a Markov Chain.
\end{definition}

One can convince yourself that a Markov Chain can have multiple $stationary$ distributions.\ An example of such Markov
Chain is when $P = Id$, which implies that any probability distribution is stationary.\ In the remaining part of this
section we will characterize a family of Markov Chains with a unique stationary distribution.

\begin{definition}
A Markov Chain with a transition matrix $P$ is called $irreducible$ iff for each pair of a source state $i$ and
a destination state $j$, it's possible to transition from $i$ to $j$ in a finite time i.e. there exists $k$ such that
$(P^k)_{ij} > 0$.
\end{definition}

\begin{definition}
A period of the $i$-th state is defined as $p(i) = gcd(\{n \geq 1 : (P^n)_{ii} > 0\}$.\ A state is called
$aperiodic$ iff $p(i) = 1$.\ A Markov Chain is called aperiodic if all of its states are aperiodic.
\end{definition}

A Markov Chain that respects the two above-mentioned properties is called an $ergodic$ Markov Chain.\ Interestingly,
$ergodic$ Markov Chains have many useful properties that leads to sophisticated algorithms.

\begin{theorem}\label{thm:one_stationary}
For any $ergodic$ Markov Chain, there exists exactly one stationary distribution $\pi$.
\end{theorem}

\begin{theorem}\label{thm:converges_to_stationary}
Let $\mu^{(0)}$ be an arbitrary initial state of any $ergodic$ Markov Chain.\ Let
$d_{TV}(\mu^{(n)}, \pi) = \frac{1}{2} \sum_{i = 1}^{k} |\mu_i^{(n)} - \pi_i|$ denote a distance between a
distribution of possible states in the $n$-th step and a stationary distribution of the defined Markov Chain.\ Then,
it holds $\lim_{n \to \infty} d_{TV}(\mu^{(n)}, \pi) = 0$.
\end{theorem}

The proofs on the theorems \ref{thm:one_stationary} and \ref{thm:converges_to_stationary} can be found in
\cite{markov_chains_book}.\ From the practical point of view, the presented theorems imply that an $ergodic$ Markov
Chain always converges to its stationary distribution, regardless of the initial distribution.\ Note that the
presented theorems do not guarantee any convergence speed.

\section{Reversible Markov Chains}

Reversible Markov Chains show up in many different areas e.g.\ they are the main building block for the Markov
Chain Monte Carlo methods.\ Formally, the reversed process is defined as $\tilde{X_t} = X_{T - t}$.\ Intuitively,
$\tilde{X}$ represents a Markov chain that is reversed in a time.

\begin{definition}
A probability distribution $\tilde{\pi}$ is said to be $reversible$ for a Markov Chain with a transition matrix $P$
iff for any $i,j$ it holds $\tilde{\pi_i} P_{ij} = \tilde{\pi_j} P_{ji}$.\ A Markov Chain is called a $reversible$
chain iff $\tilde{X_t} = X_t$ i.e. the reversed process is the same as the forward process.
\end{definition}

\begin{theorem}\label{reversible_chain}
A Markov Chain with a stationary distribution $\pi$ is reversible iff for any $i, j$ it holds
$\pi_i P_{ij} = \pi_j P_{ji}$.
\end{theorem}

The $Theorem$ \ref{reversible_chain} has many useful implications.\ First of all, in order to determine whether a
distribution is $reversible$, it suffices to check if it exists a probability measure $\tilde{\pi}$ such that
for any $i, j$ it holds $\tilde{\pi_i} P_{ij} = \tilde{\pi_j} P_{ji}$.\ What is more, in case we find such
$\tilde{\pi}$, it follows that $\pi = \tilde{\pi}$ i.e.\ the probability measure that we found is a stationary
distribution.\ This gives us a powerful tool for finding a stationary distribution.

\numberedchapter{Markov Chain Monte Carlo methods}\label{chapter:mcmc}

Consider a problem of sampling a random permutation from the set of $100$ elements.\ One can show that such space
contains $100!$ elements, implying that putting all permutations in the memory of a computer is not feasible.\ While
there exist algorithms for sampling permutations from a uniform distribution, the problem becomes harder in the case of
a skewed distribution.\ For instance, let our target distribution be defined based on positions of permutation's
elements e.g.\ $d(x) = \prod_{i = 1}^{n} e^{i * x_i}$, where $x_i$ denotes the $i$-th element of the permutation.\ While
the problem of sampling from such distribution can not be tackled using any trivial algorithm, it's possible to
simulate it with Markov Chains.\newline

In this chapter, we show how to deal with a situation in which we have a large state space and our task is to sample
an element from this space effectively.\ We will leverage the theory presented in the previous section to show how
to construct a Markov Chain with a stationary distribution equal to the target distribution we want to sample from.

\section{Metropolis–Hastings algorithm}

Let us denote the target distribution we want to sample from as $\tilde{\pi}$.\ Notice that in order to produce samples
from $\tilde{\pi}$ one may construct a Markov Chain $X$ with a transition matrix $P$ and a stationary distribution
$\pi = \tilde{\pi}$.\ Then, it suffices to simulate $X$ until we reach the stationary distribution.\ Assuming
that $X$ is $ergodic$, it can start in any initial state and based on $Theorem$ \ref{thm:converges_to_stationary} it
is guaranteed to converge to $\pi$.\ As one of our assumptions says that the space of states is very large,
constructing such Markov Chain explicitly does not solve our problem.\ Nevertheless, we can simulate such Markov Chain
with a Metropolis-Hastings algorithm.\newline

Before presenting the full algorithm, we will show how to construct a matrix $P$ such that $\tilde{\pi}$ is a
stationary distribution of a Markov Chain $X$ corresponding to $P$.\ Let us assume that we have some $proposal$
distribution $Q$ that defines a transition matrix of a Markov Chain with the same state space as $X$.\ Going further,
let us define a matrix $P$ by the following formula:\newline
$$
P_{ij} =
\begin{cases}
  Q_{ij}\min{(1, \frac{\pi_j Q_{ji}}{\pi_i Q_{ij}})} &\text{if $i \ne j$}\\
  1 - \sum\limits_{k \ne i}Q_{ik} \min{(1, \frac{\pi_k Q_{ki}}{\pi_i Q_{ik}})} &\text{otherwise}
\end{cases}
$$

While at first it may be unintuitive why $P$ would have a stationary distribution $\pi$, it can be easily
proved.\ To do so, let us show that for any $i, j$ it holds $\pi_i P_{ij} = \pi_j P_{ji}$.\ Without any loss of
generality, we can assume that $\frac{\pi_j Q_{ji}}{\pi_i Q_{ij}} \leq 1$, which implies that
$\frac{\pi_i Q_{ij}}{\pi_j Q_{ji}} \geq 1$.\ Putting everything together, we get
$$
\pi_i P_{ij} = \pi_i Q_{ij} \frac{\pi_j Q_{ji}}{\pi_i Q_{ij}} = \pi_j Q_{ji} = \pi_j P_{ji}
$$
which proves that $\pi_i P_{ij} = \pi_j P_{ji}$.\ The case when $\frac{\pi_j Q_{ji}}{\pi_i Q_{ij}} \geq 1$ is
symmetric.\ Using the $Theorem$ \ref{reversible_chain}, we can conclude that $\pi$ is the stationary distribution of
the constructed Markov Chain.\newline

Equipped with the method for generating a transition matrix $P$ for a given stationary distribution $\pi$, one may
construct an algorithm \ref{alg:metropolis_hastings} simulating a Markov Chain that corresponds to $P$.\ In case the
proposal distribution $Q$ is symmetric (i.e.\ $Q_{ij} = Q_{ji}$), the presented algorithm is called the $Metropolis$
algorithm, while in the general case of non-symmetric $Q$ it is called the $Metropolis$-$Hastings$ algorithm.\newline


\begin{algorithm}[tb]
\caption{Metropolis-Hastings}\label{alg:metropolis_hastings}
\begin{algorithmic}[1]
\State{\textbf{Input:} proposal distribution $Q$}
\State{\textbf{Input:} the current state $X_n = i$, where $n$ denotes the latest step of the simulation}
\State{Sample $j$ from a distribution $Q_i = (Q_{i1}, Q_{i2}, \dots, Q_{iN})$}
\State{Let $p = \min{(1, \frac{\pi_j Q_{ji}}{\pi_i Q_{ij}})}$}
\State{Sample $U \sim Unif(0, 1)$}
\If{$U \leq p$}
    \State{Set $X_{n + 1}$ = j}
\Else
    \State{Set $X_{n + 1} = X_n$}
\EndIf
\State{\textbf{Output:} the next state $X_{n + 1}$}
\end{algorithmic}
\end{algorithm}

\section{Gibbs sampling}\label{section:gibbs_sampling}

In this section we will show how to simulate Markov Chains for some specific distributions.\ Specifically, let us assume
that each possible state in a given state space can be represented by a function $e_i : V \to S$ that maps each vertex
$v \in V$ of an underlying graph $G = (V, K)$ to one of possible values $s \in S$.\ Generally, a state that is
represented in this form is often called a $configuration$.\ While for some distributions this
representation may not make sense, there are multiple cases for which it is the most natural way of expressing a
given distribution.\ For instance, consider a problem of colouring a graph.\ In this case, each state corresponds to a
possible graph colouring i.e.\ each vertex $v \in V$ has assigned one colour $s \in S$.\ Note that as in the graph
colouring problem it is required that two adjacent vertices are not assigned the same colour, some configurations
are not allowed.\newline

Now, assume that we are given a function $H : e \to \mathbb R_{\geq 0}$ that assigns an unnormalised probability to
each configuration $e \in E$.\ Our goal is to sample configurations according to the probabilities defined by the
function $H$.\ Let us show how to simulate a Markov chain satisfying this condition.\ To perform the $(k + 1)$-th
step of the algorithm, assuming that the current state is equal to $X_k = e_i$, first draw a vertex $\tilde{v} \in V$
uniformly and for each $s \in S$ calculate the probability
$$
P(e_{\tilde{v}} = s | e_{-\tilde{v}} = e_i) = \frac{\pi_{e_{\tilde{v}} =
s,e_{-\tilde{v}} = e_i}}{\sum_{s' \in S} \pi_{e_{\tilde{v}} = s', e_{-\tilde{v}} = e_i}}
$$

where $e_{\tilde{v}}$ denotes $s \in S$ corresponding to the vertex $\tilde{v} \in V$ and $e_{-\tilde{v}}$
denotes assignments of $s \in S$ to  all vertices $v \in V$ with $\tilde{v}$ excluded.\ The last step of the algorithm
involves sampling $\tilde{s} \in S$ from the probability distribution $P(e_{\tilde{v}} = s | e_{-\tilde{v}} = e_i)$
and setting
$$
X_{k + 1}(v) =
\begin{cases}
  \tilde{s} &\text{if $v = \tilde{v}$}\\
  X_k(v) &\text{otherwise}
\end{cases}
$$

The presented $Gibbs$ sampling algorithm is a special case of the $Metropolis$-$Hastings$ algorithm with the proposal
distribution
$$
Q_{ij} =
\begin{cases}
  \frac{1}{Z} &\text{iff $i$ is adjacent to $j$}\\
  0 &\text{otherwise}
\end{cases}
$$

which provides an instant proof of the the fact that the presented algorithm simulates a Markov Chain with the
stationary distribution $\pi$.\ An alternative proof of this fact can be found in \cite{mcmc_book}.\ The advantage of
the $Gibbs$ algorithm over the $Metropolis$-$Hastings$ algorithm is that it avoids constructing  the proposal
distribution $Q$.\ At the same time, for some cases the lack of control over $Q$ is a major downside as we will see
in the following chapters.

\numberedchapter{Ising model and its applications}

In the present chapter we will describe a model that is used by physicists to describe phase transitions.\ While
it may seem unrelated to the topic of image denoising, it turns out out that the described model can operate on
any graph.\ In particular, we will show how to interpret images as graphs and apply the Ising model to the problems
of binary and grayscale image denoising.

\section{Formal definition of the Ising model}

In this section we will follow the notions introduced in the $Section$ \ref{section:gibbs_sampling}.\ Let us consider
a graph $G = (V,\ K)$ and a set $S = \{-1,\ 1\}$.\ Now, define a configuration $e_i : V \to S$ as a function that
maps vertices of $G$ to elements from $S$ and a potential function
$$
H(e_i) = \prod\limits_{(j, k) \in K} e^{\beta e_i(v_j)e_i(v_k)}
$$

where $\beta$ is a parameter that controls the strength of $coupling$ between vertices.\ The described model is called
the $Ising$ model.\ In physics, the elements $\{-1, \, 1\}$ of the set $S$ are interpreted as a negative and positive
spin respectively.\ Assuming that $\beta > 0$, the model assigns the high probability to configurations having the
same spins assigned for neighbouring vertices.\ Note that the Ising model is uniquely determined for a given graph
$G$ and a parameter $\beta$.

\section{Applying MCMC methods to the Ising model}

Let us assume that we are given the Ising model and our task is to produce samples with probabilities proportional to
the given potential function $H$ i.e.
$$
\pi(e_i) = \frac{H(e_i)}{Z}
$$
where $Z$ is the normalising constant.\ One way of approaching this problem is to utilise MCMC methods described in
the $Chapter$ \ref{chapter:mcmc}.\newline

As we already described how to construct configurations for our model, the first algorithm that comes to mind is
the Gibbs sampler introduced in the $Section$ \ref{section:gibbs_sampling}.\ Actually, it suffices to derive a formula
for the conditional probability $P(e_{\tilde{v}} = s | e_{-\tilde{v}} = e_i)$ for any $\tilde{v} \in V$ and
$s \in \{-1,\ 1\}$.\ By expanding and simplifying this expression, we get
$$
P(e_{\tilde{v}} = -1 | e_{-\tilde{v}} = e_i) =
\frac{\pi_{e_{\tilde{v}} = -1,e_{-\tilde{v}} = e_i}}
{\pi_{e_{\tilde{v}} = -1,e_{-\tilde{v}} = e_i} + \pi_{e_{\tilde{v}} = 1,e_{-\tilde{v}} = e_i}} =
\frac{1}{1 + 2 \sum\limits_{(\tilde{v}, \tilde{w}) \in K} e_i(\tilde{w})}
$$
and utilising the fact that $|S| = 2$
$$
P(e_{\tilde{v}} = 1 | e_{-\tilde{v}} = e_i) = 1 - P(e_{\tilde{v}} = -1 | e_{-\tilde{v}} = e_i)
$$

As it was just shown, the problem of updating the spin of a vertex $\tilde{v}$ was reduced to evaluating a simple
formula.\ Furthermore, this formula can be calculated efficiently in the time complexity $\mathcal{O}(1)$ as it is
possible to store and update the information about the sum of neighbours' spins for each $v \in V$.\newline

It is easy to convince yourself that Metropolis-Hastings can also be applied to the Ising model.\ To do so, one needs
to come up with the proposal distribution $Q$ and be able to calculate the expression
$\min{(1, \frac{\pi_j Q_{ji}}{\pi_i Q_{ij}})}$ for each $i, j \in E$.\ If we define the neighbourhood of configurations
similarly as it is defined in the Gibbs sampler, the considered expression will simplify considerably. \ The
advantage of this approach is that we can construct $Q$ in a way that the simulated chain will converge faster.\newline

The final question is why do we care about sampling from the Ising model?\ To answer this question, one may observe
that some configurations are assigned much higher probability than the others.\ It means that when the simulated
Markov Chain will converge to its stationary distribution, it will start producing samples with a very high potential
defined by the function $H$.\ Actually, the most common application of the Ising model is to find those configurations
that have a high probability while rejecting the others.\ In particular, the problem of maximizing the potential
function, if we add additional constraints, is not trivial.

\section{Application to binary image denoising problem}\label{section:binary_images_problem}

Consider a problem in which we are given a binary image with some pixels flipped.\ Formally, let denote a coordinate
$(i, j)$ of our observation by $Z_{ij} \in \{0, 255\}$.\ The goal is to find an original image $X$ such that
$X_{ij} \in \{0, 255\}$, leveraging the fact that $Z$ was constructed by following the formula
$$
Z_{ij} =
\begin{cases}
  255 - X_{ij} &\text{if $U \sim Unif(0, 1) < \alpha$}\\
  X_{ij} &\text{otherwise}
\end{cases}
$$

As we are interested in images representing real objects, we can safely assume that most of the neighbouring pixels
should have the same value.\ Combining it with the fact that $Z$ was generated by flipping each pixel of $X$ with some
unknown probability $\alpha$, we can construct a model that finds $X$ providing the best balance between the
two above-mentioned constraints.\newline

Specifically, let as think of images as graphs in which the neighbourhood for a
vertex representing a coordinate $(i, j)$ is defined as $\{(i - 1, j), (i + 1, j), (i, j - 1), (i, j + 1)\}$.\ In
order to have it properly defined for pixels in the corners on an image, one may add margins.\ Now, define
$S = \{-1, 1\}$ and map our pixels by applying the following function
$$
\tilde{Z}_{ij} =
\begin{cases}
  -1 &\text{if $Z_{ij} = 0$}\\
  1 &\text{if $Z_{ij} = 255$}
\end{cases}
$$

The described model is the Ising model that assures that neighbouring pixels have the same values.\ In order to
take into account the second constraint, one may increase the potential of configurations that have their pixel values
equal to the observed values defined by $Z$.\ Putting it all together, we get the following potential function:
$$
H(e_i) = \prod\limits_{(j, k) \in K} e^{\beta e_i(v_j)e_i(v_k)}
\prod\limits_{j \in V} \mathbbm{1}_{e_i(v_j) = \tilde{Z}(v_j)}(1 - \alpha) +
\mathbbm{1}_{e_i(v_j) \neq \tilde{Z}(v_j)}\alpha
$$

where $\alpha, \beta$ are the parameters controlling the observation strength and the coupling strength respectively.
Intuitively, increasing the observation strength causes our model to put more weight to the observed pixels in $Z$,
while increasing the coupling strength enforces the neighbouring pixels to have the same values.

\section{Extension to grayscale images}

In the present section we show how to extend the model described in the $Section$ \ref{section:binary_images_problem}
to grayscale images.\ Formally, given an observation $Z$ such that $Z_{ij} \in \{0, 1, \dots, 255\}$ the goal is to
find an original image $X$.\ In this paper, we assume that $Z$ was generated by flipping random pixels of $X$ i.e.
$$
Z_{ij} =
\begin{cases}
  255 - X_{ij} &\text{if $U \sim Unif(0, 1) < \alpha$}\\
  X_{ij} &\text{otherwise}
\end{cases}
$$
and leave more general case for the future work.\ Similarly to the case of binary images, our optimization goal is to
maximise the potential function that takes into account the observation $Z$ and the coupling between neighbouring
pixels.\ While there are many ways to define such potential function $H$, we find that the following function
$$
H(e_i) = \prod\limits_{(j, k) \in K} e^{-\beta |e_i(v_j) - e_i(v_k)|}
\prod\limits_{j \in V} \mathbbm{1}_{e_i(v_j) = \tilde{Z}(v_j)}(1 - \alpha) +
\mathbbm{1}_{e_i(v_j) \neq \tilde{Z}(v_j)}\alpha
$$
expresses our constraints in efficient and intuitive way.\ In particular, the penalty given by the coupling part of
the function $H$ is proportional to the sum of distances between a considered pixel and all its neighbours.\ As a
result, the model is encouraged to flip those pixels that are significantly different from all of its
neighbours.\ The other observation is that $H$ is defined in a way that most of its components are equal for the
neighbouring configurations, making the update rule of the Gibbs algorithm simple and efficient.

\numberedchapter{Gradient-based image denoising}

TODO

\section{Proposal distribution inspired by gradients}

TODO

\section{Efficient implementation for image denoising problem}

TODO

\numberedchapter{Experiments: binary images}

TODO

\section{Methodology}

TODO

\section{Denoising quality based on image size}

TODO

\section{Denoising quality based on noise level}

TODO

\section{Sensitivity of noise level prior parameter}

TODO

\section{Qualitative results}

TODO

\numberedchapter{Experiments: grayscale images}

TODO

\section{Methodology}

TODO

\section{Denoising quality}

TODO

\section{Qualitative results}

TODO

\numberedchapter{Summary}

TODO

Podstawowym problemem, z jakim zmagają się wszyscy przedsiębiorcy, jest przygotowa- nie oferty w ten sposób, by była ona zarówno atrakcyjna dla klienta (a przynajmniej bardziej atrakcyjna niż to, co oferuje konkurencja), jak i przynosiła zyski dla przedsiębiorcy (a już na pewno nie generowała strat). Branża ubezpieczeniowa nie jest tu wyjątkiem. Każda ubez- pieczalnia zatrudnia sztab aktuariuszy, których zadaniem jest wyznaczenie obowiązkowych składek w ten.
Computers \cite{einstein} sdsd.
Podstawowym problemem, z jakim zmagają się wszyscy przedsiębiorcy, jest przygotowa- nie oferty w ten sposób, by była ona zarówno atrakcyjna dla klienta (a przynajmniej bardziej atrakcyjna niż to, co oferuje konkurencja), jak i przynosiła zyski dla przedsiębiorcy (a już na pewno nie generowała strat). Branża ubezpieczeniowa nie jest tu wyjątkiem. Każda ubez- pieczalnia zatrudnia sztab aktuariuszy, których zadaniem jest wyznaczenie obowiązkowych składek w ten sposób, by pokryły koszta działalności firmy oraz przyszłe zobowiązania wzglę- dem klientów. W przypadku ubezpieczeń majątkowych zadanie jest o tyle trudniejsze, że bardzo istotny jest tak zwany czynnik ludzki czyli np. skłonność do ryzyka lub umiejętność prowadzenia pojazdów. Są to parametry trudne do zmierzenia, stąd potrzeba jak najlepsze- go modelowania wpływu tych składowych na wartość zgłaszanych szkód, na skutek których konieczna będzie wypłata odszkodowania. Obecnie najczęściej stosowaną metodą jest sys- tem Bonus - Malus, którego głównym założeniem jest związanie wysokości składki z liczbą wypadków spowodowanych przez kierowcę. Wadą tego rozwiązania jest nieuwzględnianie istot- nych czynników takich jak np. częstotliwość prowadzenia pojazdu czy styl jazdy. Pomysłem na rozwiązanie tego problemu jest montowanie w samochodach urządzeń rejestrujących jaz- dę, jednak może się to spotkać z oporem klientów dbających o ochronę swojej prywatności. Powstaje więc pytanie jak poprawić stosowane rozwiązania bez konieczności zbierania dodat- kowych danych o kierowcy. Odpowiedzią na to pytanie mogą być tak zwane ukryte modele Markowa.
Figure \ref{fig:xxx} compared with Figure \ref{fig:yyy} sss \cite{einstein} sdsd.


\begin{figure}
\centering
\includegraphics[scale=0.4]{binary_input_size_plots}
\caption{Example of a parametric plot ($\sin (x), \cos(x), x$)}
\label{fig:xxx}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.4]{binary_input_size_plots}
\caption{Example of a parametric plot ($\sin (x), \cos(x), x$)}
\label{fig:yyy}
\end{figure}

\begin{thebibliography}{9}

\bibitem{markov_chains_book}
Olle Häggström - \textit{Finite Markov chains and algorithmic applications (2002)}

\bibitem{mcmc_book}
Pierre Bremaud - \textit{Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues (1999)}

\end{thebibliography}

\end{document}
